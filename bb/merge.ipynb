{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import urllib.request\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SDT_YY</th>\n",
       "      <th>SDT_MM</th>\n",
       "      <th>SDT_DD</th>\n",
       "      <th>SDT_DY</th>\n",
       "      <th>ARP</th>\n",
       "      <th>ODP</th>\n",
       "      <th>FLO</th>\n",
       "      <th>FLT</th>\n",
       "      <th>REG</th>\n",
       "      <th>AOD</th>\n",
       "      <th>IRR</th>\n",
       "      <th>STT</th>\n",
       "      <th>ATT</th>\n",
       "      <th>DLY</th>\n",
       "      <th>DRR</th>\n",
       "      <th>CNL</th>\n",
       "      <th>CNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>일</td>\n",
       "      <td>ARP1</td>\n",
       "      <td>ARP3</td>\n",
       "      <td>A</td>\n",
       "      <td>A1901</td>\n",
       "      <td>SEw3Nzc2</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>6:10</td>\n",
       "      <td>6:18</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>일</td>\n",
       "      <td>ARP1</td>\n",
       "      <td>ARP3</td>\n",
       "      <td>A</td>\n",
       "      <td>A1905</td>\n",
       "      <td>SEw4MjM2</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>6:15</td>\n",
       "      <td>6:25</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>일</td>\n",
       "      <td>ARP1</td>\n",
       "      <td>ARP3</td>\n",
       "      <td>L</td>\n",
       "      <td>L1751</td>\n",
       "      <td>SEw4MjM3</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>6:20</td>\n",
       "      <td>6:30</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>일</td>\n",
       "      <td>ARP1</td>\n",
       "      <td>ARP3</td>\n",
       "      <td>F</td>\n",
       "      <td>F1201</td>\n",
       "      <td>SEw4MjA3</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>6:25</td>\n",
       "      <td>6:34</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>일</td>\n",
       "      <td>ARP3</td>\n",
       "      <td>ARP1</td>\n",
       "      <td>A</td>\n",
       "      <td>A1900</td>\n",
       "      <td>SEw3NzAz</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>6:30</td>\n",
       "      <td>6:37</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SDT_YY  SDT_MM  SDT_DD SDT_DY   ARP   ODP FLO    FLT       REG AOD IRR  \\\n",
       "0    2017       1       1      일  ARP1  ARP3   A  A1901  SEw3Nzc2   D   N   \n",
       "1    2017       1       1      일  ARP1  ARP3   A  A1905  SEw4MjM2   D   N   \n",
       "2    2017       1       1      일  ARP1  ARP3   L  L1751  SEw4MjM3   D   N   \n",
       "3    2017       1       1      일  ARP1  ARP3   F  F1201  SEw4MjA3   D   N   \n",
       "4    2017       1       1      일  ARP3  ARP1   A  A1900  SEw3NzAz   D   N   \n",
       "\n",
       "    STT   ATT DLY  DRR CNL  CNR  \n",
       "0  6:10  6:18   N  NaN   N  NaN  \n",
       "1  6:15  6:25   N  NaN   N  NaN  \n",
       "2  6:20  6:30   N  NaN   N  NaN  \n",
       "3  6:25  6:34   N  NaN   N  NaN  \n",
       "4  6:30  6:37   N  NaN   N  NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin = pd.read_csv('AFSNT.csv', encoding=\"cp949\")\n",
    "origin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시간전처리와 요일 전처리 빼고는 model 돌리기 전에 하기!\n",
    "\n",
    "# 시간에서 시 데이터만 추출 ==> 분은 영향을 줄임.\n",
    "origin['ATT'] = pd.to_datetime(origin['ATT'],format= '%H:%M').dt.hour\n",
    "origin['STT'] = pd.to_datetime(origin['STT'],format= '%H:%M').dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin.rename(columns={'SDT_YY':'Year', 'SDT_MM':'Month', 'SDT_DD':'DAY'}, inplace=True)\n",
    "\n",
    "\n",
    "#요일 카테고리컬\n",
    "one_hot_dy = pd.get_dummies(origin['SDT_DY'])\n",
    "origin = origin.drop(['SDT_DY'],axis = 1)\n",
    "origin = origin.join(one_hot_dy)\n",
    "origin.rename(columns={\"일\":\"Sun\",\"월\":\"Mon\",\"화\":\"Tue\",\"수\":\"Wed\",\"목\":\"Thu\",\"금\":\"Fri\",\"토\":\"SAT\",\"일\":\"Sun\"                    \n",
    "                  }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 987709 entries, 0 to 987708\n",
      "Data columns (total 23 columns):\n",
      "Year     987709 non-null int64\n",
      "Month    987709 non-null int64\n",
      "DAY      987709 non-null int64\n",
      "ARP      987709 non-null object\n",
      "ODP      987709 non-null object\n",
      "FLO      987709 non-null object\n",
      "FLT      987709 non-null object\n",
      "REG      987251 non-null object\n",
      "AOD      987709 non-null object\n",
      "IRR      987709 non-null object\n",
      "STT      987709 non-null int64\n",
      "ATT      979461 non-null float64\n",
      "DLY      987709 non-null object\n",
      "DRR      119917 non-null object\n",
      "CNL      987709 non-null object\n",
      "CNR      3018 non-null object\n",
      "Fri      987709 non-null uint8\n",
      "Thu      987709 non-null uint8\n",
      "Wed      987709 non-null uint8\n",
      "Mon      987709 non-null uint8\n",
      "Sun      987709 non-null uint8\n",
      "SAT      987709 non-null uint8\n",
      "Tue      987709 non-null uint8\n",
      "dtypes: float64(1), int64(4), object(11), uint8(7)\n",
      "memory usage: 127.2+ MB\n"
     ]
    }
   ],
   "source": [
    "origin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1일 이라고 되어있는 데이터를 합칠떄 01로 변환\n",
    "def changeDate(data):\n",
    "    data=str(data)\n",
    "    if len(data)==1:\n",
    "        data=\"0\"+data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#군공항 제외날씨 데이터 다운(항공데이터)\n",
    "\n",
    "def downloadAirport(yy,mm,area):\n",
    "    mm=changeDate(mm)\n",
    "    yy=str(yy)\n",
    "    url='http://amoapi.kma.go.kr/amoApi/air_stcs?icao='+area+'&yyyymm='+yy+mm\n",
    "    response = urllib.request.urlopen(url)\n",
    "    cr = csv.reader(codecs.iterdecode(response, 'utf-8'))\n",
    "    \n",
    "    #url로 읽어와 데이터 프레임에 저장\n",
    "    temp=[]\n",
    "    for line in cr:\n",
    "        temp.append(line)\n",
    "    \n",
    "    labels=temp[0]\n",
    "    weather=pd.DataFrame.from_records(temp[1:],columns=labels)\n",
    "    \n",
    "    weather[\"TM\"]=weather[\"TM\"].astype(\"str\")\n",
    "\n",
    "    weather[\"Year\"]=weather[\"TM\"].str.slice(0,4)\n",
    "    weather[\"Month\"]=weather[\"TM\"].str.slice(4,6)\n",
    "    weather[\"DAY\"]=weather[\"TM\"].str.slice(6,8)\n",
    "    hh=weather[\"TM\"].str.slice(8,10)    \n",
    "\n",
    "    #weather['DAY'] = pd.to_datetime(weather[['Year', 'Month', 'DAY']])\n",
    "    weather['STT']=hh\n",
    "    weather['STT']= weather['STT'].astype('int')\n",
    "    weather['STT']=weather['STT'].replace(24,0)\n",
    "    \n",
    "    \n",
    "    weather.drop(columns=[\"TM\"], axis=1, inplace=True)\n",
    "    weather.drop(columns=[\"WD\",\"WS_GST\",\"RVR1\",\"RVR2\",\"RVR3\",\"RVR4\",\"CLA_1LYR\"\n",
    "               ,\"BASE_1LYR\",\"CLF_1LYR\",\"CLA_2LYR\",\"BASE_2LYR\",\"CLF_2LYR\",\n",
    "               \"CLA_3LYR\",\"BASE_3LYR\",\"CLF_3LYR\",\"CLA_4LYR\",\"BASE_4LYR\",\"CLF_4LYR\"], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    #기록 안되어있는거는 0\n",
    "    weather=weather.fillna(0)\n",
    "    weather[\"Year\"]=weather[\"Year\"].astype(\"int\")\n",
    "    weather[\"Month\"]=weather[\"Month\"].astype(\"int\")\n",
    "    weather[\"DAY\"]=weather[\"DAY\"].astype(\"int\")\n",
    "    \n",
    "    return weather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#군공항 데이터 저장(기상청 csv)\n",
    "def downloadWeather(year):\n",
    "    filename=\"data/\"+str(year)+\".csv\"\n",
    "    weather=pd.read_csv(filename,encoding=\"cp949\")\n",
    "    \n",
    "    #일시를 데이트타입과 시간을 따로 저장\n",
    "    weather[\"일시\"] = weather[\"일시\"].astype('str')\n",
    "    date=weather[\"일시\"].str.split(expand=True)\n",
    "    day=date[0].str.split(\"-\",expand=True)\n",
    "    \n",
    "    weather[\"Year\"]=day[0]\n",
    "    weather[\"Month\"]=day[1]\n",
    "    weather[\"DAY\"]=day[2]\n",
    "    \n",
    "    \n",
    "    weather[\"STT\"]=date[1]\n",
    "    \n",
    "    #일시 drop(변경 전 데이터)\n",
    "    weather.drop(columns=['일시'], axis=1, inplace=True)\n",
    "    weather = weather.drop(['지면온도(°C)',\"지면온도 QC플래그\",\"5cm 지중온도(°C)\",\"10cm 지중온도(°C)\",\"20cm 지중온도(°C)\",\"기온 QC플래그\",\n",
    "                            \"강수량 QC플래그\",\"풍속 QC플래그\",\"풍향(16방위)\",\"풍향 QC플래그\",\"습도 QC플래그\",\"현지기압 QC플래그\",\n",
    "                            \"해면기압 QC플래그\",\"중하층운량(10분위)\",\"운형(운형약어)\",\"최저운고(100m )\",\"지면상태(지면상태코드)\",\n",
    "                           \"적설(cm)\",\"3시간신적설(cm)\", \"30cm 지중온도(°C)\",\"일조(hr)\",\"일조 QC플래그\",\"일사(MJ/m2)\"],axis = 1)\n",
    "    weather.rename(columns={\"지점\" : \"area\", \"기온(°C)\":\"temp\" ,\"강수량(mm)\":\"rain\", \"풍속(m/s)\":\"windSpeed\",\n",
    "                            \"습도(%)\":\"hum\",\"증기압(hPa)\":\"Vapor\",\"이슬점온도(°C)\":\"dew\",\"현지기압(hPa)\":\"hpa\",\n",
    "                            \"해면기압(hPa)\":\"seeHpa\",\"시정(10m)\":\"visible\",\"전운량(10분위)\":\"cloudTotal\",\n",
    "                            \"현상번호(국내식)\":\"weatherCode\"}, inplace=True)\n",
    "    weather[\"STT\"]=weather[\"STT\"].astype(\"str\")\n",
    "    weather[\"STT\"]=weather[\"STT\"].str.split(\":\",expand=True)[0]\n",
    "    weather[\"STT\"]=weather[\"STT\"].astype(\"int\")\n",
    "    \n",
    "    weather[\"Year\"]=weather[\"Year\"].astype(\"int\")\n",
    "    weather[\"Month\"]=weather[\"Month\"].astype(\"int\")\n",
    "    weather[\"DAY\"]=weather[\"DAY\"].astype(\"int\")\n",
    "    \n",
    "    weather=weather.fillna(0)\n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 군공항X(공항공사 데이터 돌리기)\n",
    "def mergeAirportData():\n",
    "    df_all=pd.DataFrame()\n",
    "    temp=origin[(origin.ARP==\"ARP1\") | (origin.ARP==\"ARP3\")| (origin.ARP==\"ARP5\") | (origin.ARP==\"ARP7\") \n",
    "                | (origin.ARP==\"ARP9\")| (origin.ARP==\"ARP10\") ]\n",
    "    elements,count=np.unique(temp[\"ARP\"],return_counts=True)\n",
    "    for i in range(len(elements)):\n",
    "        df_areaD=pd.DataFrame()\n",
    "        arp=elements[i]\n",
    "        area={\"ARP1\":\"RKSS\",\"ARP3\":\"RKPC\",\"ARP5\":\"RKPU\",\n",
    "              \"ARP7\":\"RKJB\",\"ARP9\":\"RKJY\",\"ARP10\":\"RKNY\"}.get(arp)\n",
    "        df_areaD=temp[temp[\"ARP\"]==arp]\n",
    "        \n",
    "        for j in range(3):\n",
    "            year=2017+j\n",
    "            for k in range(12):\n",
    "                ## 2019년은 7월데이터 X\n",
    "                if year==2019:\n",
    "                    if k>=6:\n",
    "                        break;\n",
    "                month=1+k\n",
    "                df_date=df_areaD[(df_areaD[\"Year\"]==year) & (df_areaD[\"Month\"]==month)]\n",
    "                weather=downloadAirport(year,month,area)\n",
    "                df_new=pd.merge(df_areaD,weather,on=[\"Year\",\"Month\",\"DAY\",\"STT\"])\n",
    "                \n",
    "                if i==0 and j==0 and k==0:\n",
    "                    df_all=df_new.copy()\n",
    "                else:\n",
    "                    df_all=df_all.append(df_new)\n",
    "                    \n",
    "    #기상청데이터와 연결 하기 위해 rename                \n",
    "    df_all.rename(columns={'WSPD':'windSpeed','VIS':\"visible\",\"TMP\":\"temp\",\n",
    "                      \"TD\":\"dew\",'PS':'hpa','PA':'seeHpa','RN':'rain','HM':'hum',\n",
    "                        'CA_TOT':'cloudTotal','WC':\"weatherCode\"}, inplace=True)\n",
    "    \n",
    "    \n",
    "    #기상청 데이터와 단위 맞추기\n",
    "    df_all['temp']=df_all['temp'].astype(\"float\")/10\n",
    "    df_all['hpa']=df_all['hpa'].astype(\"float\")/10\n",
    "    df_all['seeHpa']=df_all['seeHpa'].astype(\"float\")/10\n",
    "    df_all['dew']=df_all['dew'].astype(\"float\")/10\n",
    "    df_all['windSpeed']=df_all['windSpeed'].astype(\"float\")*1852/3600\n",
    "    \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeWeatherData():\n",
    "    temp=origin[(origin.ARP==\"ARP2\") | (origin.ARP==\"ARP4\")| (origin.ARP==\"ARP6\") | (origin.ARP==\"ARP8\") | (origin.ARP==\"ARP11\")\n",
    "                | (origin.ARP==\"ARP12\")| (origin.ARP==\"ARP13\")| (origin.ARP==\"ARP14\")|(origin.ARP==\"ARP15\")]\n",
    "    elements,count=np.unique(temp[\"ARP\"],return_counts=True)\n",
    "    \n",
    "    for i in range(3):\n",
    "        df_areaD=pd.DataFrame()\n",
    "        year=2017+i\n",
    "        weather=downloadWeather(year)\n",
    "        df_yearD=origin[origin[\"Year\"]==year]\n",
    "        for j in range(len(elements)):\n",
    "            arp=elements[j]\n",
    "            area={\"ARP2\":159,\"ARP4\":143,\"ARP6\":131, \"ARP8\":156,\n",
    "                  \"ARP11\":138,\"ARP12\":192, \"ARP13\":140,\"ARP14\":114,\"ARP15\":112}.get(arp)\n",
    "            \n",
    "            df_areaD=df_yearD[df_yearD[\"ARP\"]==arp]\n",
    "            df_new=pd.DataFrame()\n",
    "            weatherT=weather[weather[\"area\"]==area]\n",
    "           \n",
    "            df_new=pd.merge(df_areaD,weatherT,on=[\"DAY\",\"STT\",\"Year\",\"Month\"])\n",
    "        \n",
    "            if i==0 and j==0:\n",
    "                df_all=df_new.copy()\n",
    "            else:\n",
    "                df_all=df_all.append(df_new)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport=mergeAirportData()\n",
    "df_nonAirport=mergeWeatherData()\n",
    "\n",
    "df_weather=df_airport.append(df_nonAirport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.drop(['area','Vapor'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.to_csv(\"weatherFinal.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('weatherFinal.csv', encoding=\"cp949\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 코드 추가(나중에 한 파일로 낼꺼 생각해서)\n",
    "\n",
    "##전처리 코드 추가\n",
    "df = df[df.IRR != \"Y\"] # 부정기 없애기 \n",
    "df = df[df.CNL != \"Y\"]\n",
    "\n",
    "# 비행기 취소와 관련된 Column 삭제\n",
    "df.drop(columns=['CNL', 'CNR'], axis=1, inplace=True)\n",
    "\n",
    "# 사용되지 않을 것 같은 데이터 일단 삭제\n",
    "df.drop(columns=['REG', 'IRR'], axis=1, inplace=True)\n",
    "\n",
    "# 딜레이 이유 (나중에 쓰일 듯)\n",
    "df.drop(columns=['DRR'], axis=1, inplace=True)\n",
    "\n",
    "############# 날씨 데이터 추가 후 주석 제거할 것.\n",
    "# 날씨 관련 안쓰는 feature 삭제\n",
    "# df.drop(columns=['rain', 'weatherCode'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATT (actual time data)가 널 값인 레코드 삭제 \n",
    "df = df[pd.notnull(df['ATT'])]\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARP와 ODP가 같은 데이터 --> Wrong data => 삭제\n",
    "df.drop(df[df['ARP'] == df['ODP']].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARP 경로 파생변수 생성\n",
    "df['ARPODP'] = df['ARP'] + '_' + df['ODP']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "df['Diff'] = (pd.to_datetime(df['ATT'],format= '%H:%M') - pd.to_datetime(df['STT'],format= '%H:%M')).dt.seconds.astype('int64')\n",
    "\n",
    "# STT와 ATT 격차 큰 순대로 정렬\n",
    "df = df.sort_values(by=['Diff'], ascending=False)\n",
    "\n",
    "########################################################################출발\n",
    "# 딜레이가 최대 5시간이라고 가정했을 때, --> 즉, 2시간 초과한 딜레이는 wrong값이라 가정\n",
    "max_delay_hour = 5\n",
    "max_delay = max_delay_hour * 3600 # seconds\n",
    "\n",
    "# 출발비행기의 경우, 조금이라도 출발이 빠른 건 wrong data라 판단.\n",
    "# 7200보다 큰 값을 가지는 Diff 데이터 wrong 값 처리\n",
    "df = df[((df['Diff'] <= max_delay) & (df['AOD']=='D')) | (df['AOD']=='A')]\n",
    "\n",
    "df.head(100)\n",
    "\n",
    "########################################################################도착\n",
    "#이정도는 늦게 도착해도 O\n",
    "#2시간은 예상보다 늦게도착할 수 있다. 그 이상은 말이안된다\n",
    "max_delay_hour_arr = 5\n",
    "max_delay_arr = max_delay_hour_arr * 3600 # seconds\n",
    "\n",
    "#몇분 일찍도착해도 O\n",
    "#30분은 예상보다 빨리도착할 수 있음. 그거보다 빨리도착하는 건 말이 안됨\n",
    "min_delay = 30*60\n",
    "min_delay = 86400 - min_delay  # 86400(24시간)보다 위인거만 살려놓기\n",
    "df = df[(df['AOD']=='D') |((df['Diff'] <= max_delay_arr) & (df['AOD']=='A')) | ((df['AOD']=='A')& (df['Diff'] >= min_delay )) ]\n",
    "df.loc[df['Diff'] >=min_delay, 'Diff'] = df.loc[df['Diff'] >=min_delay, 'Diff']  - 86400\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the numerical data\n",
    "numerical_feature = [col for col in df.columns if df[col].dtypes == 'int64']\n",
    "print(numerical_feature)\n",
    "\n",
    "def dist_box(df, feature_list):\n",
    "    for col in feature_list:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        sns.distplot(df.loc[df[col].notnull(), col])\n",
    "        plt.title(col)\n",
    "        plt.show()\n",
    "        \n",
    "        df[col].plot(kind='box', color='red')\n",
    "        plt.show()\n",
    "\n",
    "\"\"\"\n",
    "print('*'*50)\n",
    "print('All')\n",
    "dist_box(df, numerical_feature)\n",
    "print('*'*50)\n",
    "print('Arrive')\n",
    "dist_box(df[df['AOD'] == 'A'], numerical_feature)\n",
    "print('*'*50)\n",
    "print('Departure')\n",
    "dist_box(df[df['AOD'] == 'D'], numerical_feature)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data --> one hot encoding\n",
    "\n",
    "def one_hot_dummies(df, *args):\n",
    "    for col in args:\n",
    "        one_hot_col = pd.get_dummies(df[col])\n",
    "        df = df.drop([col], axis = 1)\n",
    "        \n",
    "        try:\n",
    "            df = df.join(one_hot_col)\n",
    "        except:\n",
    "            one_hot_col.rename(columns={'ARP1':'ARP1_','ARP2':'ARP2_','ARP3':'ARP3_','ARP4':'ARP4_','ARP5':'ARP5_',\n",
    "                                        'ARP14':'ARP14_','ARP12':'ARP12_','ARP10':'ARP10_','ARP8':'ARP8_','ARP6':'ARP6_',\n",
    "                                        'ARP15':'ARP15_','ARP13':'ARP13_','ARP11':'ARP11_','ARP9':'ARP9_','ARP7':'ARP7_'},\n",
    "                               inplace=True)\n",
    "            df = df.join(one_hot_col)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = one_hot_dummies(df, 'ARP', 'ODP', 'FLO', 'ARPODP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[['Month', 'STT']] = scaler.fit_transform(df[['Month', 'STT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target 레이블링\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# DLY도 1과 0으로 데이터 처리\n",
    "le = preprocessing.LabelEncoder()\n",
    "df[['DLY']] = le.fit_transform(df[['DLY']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도착, 출발 데이터 분리\n",
    "df_A = df[df['AOD']=='A']\n",
    "df_D = df[df['AOD']=='D']\n",
    "\n",
    "# AOD column삭제 \n",
    "df_A = df_A.drop(['AOD'],axis = 1)\n",
    "df_D = df_D.drop(['AOD'],axis = 1)\n",
    "\n",
    "df_A.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTestSet(df):\n",
    "    X = df.drop(['DLY'], axis = 1)\n",
    "    y = df['DLY']\n",
    "        \n",
    "    X_tr, X_t, y_tr, y_t = train_test_split(X,y, test_size= 0.3, random_state = 42)\n",
    "    \n",
    "    print(\"X_train set--------------------\")\n",
    "    print(\"Shape:\",X_tr.shape)\n",
    "    print(\"Target:\")\n",
    "    print(y_tr.value_counts())\n",
    "    print()\n",
    "      \n",
    "    print(\"X_test set info-----------------\")\n",
    "    print(\"Shape:\",X_t.shape)\n",
    "    print(\"Target:\")\n",
    "    print(y_t.value_counts())\n",
    "    print()\n",
    "\n",
    "    return [X_tr, X_t, y_tr, y_t]\n",
    "\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = makeTestSet(df_A)\n",
    "X_train_D, X_test_D, y_train_D, y_test_D = makeTestSet(df_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------변수 중요도 확인하고 상위 OO개 남기기 -------------------\n",
    "\n",
    "def feature_importance(X_train, y_train, X_test, y_test):\n",
    "        \n",
    "    #### Skew Data처리할거면 주석 해제하기!!\n",
    "    #X_train, y_train = imbalance(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    log_rg = LogisticRegression().fit(X_train, y_train)\n",
    "    cross_val_score(log_rg, X_train, y_train, cv=5)\n",
    "    log_rg.score(X_test, y_test)\n",
    "    print(classification_report(y_test, log_rg.predict(X_test)))\n",
    "\n",
    "    # X column 개수 출력\n",
    "    #print(len(X.columns)) \n",
    "\n",
    "    # 변수 중요도 \n",
    "    fi = pd.DataFrame(zip(X_train.columns.values, abs(log_rg.coef_.ravel())))\n",
    "    fi.columns = ['feature', 'coef']\n",
    "    fi.sort_values(\"coef\", ascending=False, inplace=True)\n",
    "    fi = fi.reset_index().drop(['index'], axis=1)\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        print(fi)\n",
    "\n",
    "    for index, val in enumerate(fi.iloc[:, 1]):\n",
    "        if val < 0.1:\n",
    "            X_train.drop([fi.iloc[index, 0]], axis = 1, inplace = True)\n",
    "            X_test.drop([fi.iloc[index, 0]], axis = 1, inplace = True)   \n",
    "    return\n",
    "\n",
    "feature_importance(X_train_A, y_train_A, X_test_A, y_test_A)\n",
    "feature_importance(X_train_D, y_train_D, X_test_D, y_test_D)\n",
    "print(len(X_train_A.columns))\n",
    "print(len(X_test_A.columns))\n",
    "print(len(X_train_D.columns))\n",
    "print(len(X_test_D.columns))\n",
    "\n",
    "#--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalance (X_train, y_train):\n",
    "\n",
    "    # 모델설정\n",
    "    sm = SMOTE(ratio='auto', kind='regular')\n",
    "\n",
    "    # train데이터를 넣어 복제함\n",
    "    X_resampled, y_resampled = sm.fit_sample(X_train,list(y_train))\n",
    "\n",
    "    print('After OverSampling, the shape of train_X: {}'.format(X_resampled.shape))\n",
    "    print('After OverSampling, the shape of train_y: {} \\n'.format(X_resampled.shape))\n",
    "\n",
    "    print(\"After OverSampling, counts of label '1': {}\".format(sum(y_resampled==1)))\n",
    "    print(\"After OverSampling, counts of label '0': {}\".format(sum(y_resampled==0)))\n",
    "    \n",
    "    return [X_resampled, y_resampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve그리기\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_heatmap(y_true, y_pred):\n",
    "    data = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.set(font_scale=1.4)#for label size\n",
    "    sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel (X_train, y_train, X_test, y_test, base = True):\n",
    "    \n",
    "    #X_train, y_train = underSampling(X_train, y_train)\n",
    "    \n",
    "     ## Skew Data처리할거면 주석 풀기!!\n",
    "    X_train, y_train = imbalance(X_train, y_train)\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    if base ==True:\n",
    "        models.append(('RF', RandomForestClassifier(max_depth=6, n_estimators=1000, random_state=0, criterion='entropy')))\n",
    "    else:\n",
    "#         models.append(('LR', LogisticRegression()))\n",
    "#         models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "#         models.append(('KNN', KNeighborsClassifier()))\n",
    "#         models.append(('CART', DecisionTreeClassifier()))\n",
    "#         models.append(('NB', GaussianNB()))\n",
    "        models.append(('RF', RandomForestClassifier(max_depth=4, n_estimators=100, random_state=0)))\n",
    "#         models.append(('SVM', SVC(gamma='auto')))\n",
    "#         models.append(('XGB', XGBClassifier()))\n",
    "\n",
    "    \n",
    "    # 평가\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = 'recall'\n",
    "\n",
    "    seed = 7\n",
    "    for name, model in models:\n",
    "        # K-Fold\n",
    "#         kfold = KFold(n_splits=10, random_state=seed)\n",
    "#         cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "#         results.append(cv_results)\n",
    "#         names.append(name)\n",
    "#         msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#         print(msg)\n",
    "\n",
    "        # Hold out \n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = pd.Series(model.predict(X_test))\n",
    "\n",
    "        # Resets index to compare original test data with predicted data\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        y_predict = y_predict.reset_index(drop=True)\n",
    "\n",
    "#         plt.figure(figsize=(15, 5))\n",
    "#         plt.scatter(range(y_test.shape[0]), y_test, c='gray')\n",
    "#         plt.scatter(range(y_predict.shape[0]), y_predict , c='r')\n",
    "#         diff = abs(y_test - y_predict)\n",
    "#         plt.bar(range(diff.shape[0]), diff, color='gray')\n",
    "#         plt.title('Result - Original comparsion')\n",
    "#         plt.legend(['Original', 'Predict'])\n",
    "#         plt.show()\n",
    "\n",
    "        print(model.score(X_test, y_test))\n",
    "        print('-' * 50)\n",
    "        \n",
    "        #--------ROC Curve-----------------\n",
    "        probs = model.predict_proba(X_test)\n",
    "        probs = probs[:, 1]\n",
    "        auc = roc_auc_score(y_test, probs)\n",
    "        print('AUC: %.2f' % auc)\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "        plot_roc_curve(fpr, tpr)\n",
    "        #-----------------------------------\n",
    "        \n",
    "        #-------- Confusion matrix heatmap -----------------\n",
    "        confusion_matrix_heatmap(y_test, y_predict)\n",
    "        print(classification_report(y_test, y_predict))\n",
    "        #-----------------------------------\n",
    "        \n",
    "             \n",
    "#      # boxplot algorithm comparison\n",
    "#     fig = plt.figure(figsize=(11,6))\n",
    "#     fig.suptitle('Algorithm Comparison')\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     plt.boxplot(results)\n",
    "#     ax.set_xticklabels(names)\n",
    "#     plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True면 base algorithm만 실행하겠다 (base algorithm : Random Forest)\n",
    "# False면 모든 알고리즘을 실행하겠다.\n",
    "runModel (X_train_D,y_train_D, X_test_D, y_test_D, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True면 base algorithm만 실행하겠다 (base algorithm : Random Forest)\n",
    "# False면 모든 알고리즘을 실행하겠다.\n",
    "runModel (X_train_A,y_train_A, X_test_A, y_test_A, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#우리가 예측해야 할 데이터\n",
    "dly = pd.read_csv('AFSNT_DLY.CSV', encoding=\"cp949\")\n",
    "dly.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요없는 column 제거\n",
    "dly.drop(columns=['FLT', 'DLY', 'DLY_RATE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파생변수\n",
    "dly['ARPODP'] = dly['ARP'] + '_' + dly['ODP']\n",
    "# 분 삭제\n",
    "dly['STT'] = pd.to_datetime(dly['STT'],format= '%H:%M').dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import pandas as pd\n",
    "import dateutil.parser\n",
    "#날씨데이터 json 읽어오는 함수\n",
    "\n",
    "#나중에 9월 1일을 9월 16일로 바꾼 후 실행..\n",
    "#(읽어오는거 확인용으로 9월 1일 부터...)\n",
    "def readJSON(area):\n",
    "    \n",
    "    request = urllib.request.urlopen('https://api.aerisapi.com/forecasts/'+area+',korea?from=09/16/2019&format=json&filter=1hr&limit=999&client_id=gHOhinKWCL1fwDUpI1Ec7&client_secret=TRamBTyXpORXcmDKVqc2S4i4mnCjVxxHMt6cllui')\n",
    "    response = request.read()\n",
    "    data= json.loads(response)\n",
    "\n",
    "\n",
    "    if data['success']:\n",
    "        return data\n",
    "\n",
    "    else:\n",
    "        print(\"An error occurred: %s\" % (data['error']['description']))\n",
    "        return \"\"\n",
    "        request.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#날씨 csv 저장\n",
    "def storeWeather():\n",
    "    futureweather=pd.DataFrame(columns=['SDT_YY', 'SDT_MM', 'SDT_DD','STT','ARP','temp', 'hum', 'dew', 'windSpeed','hpa'])\n",
    "\n",
    "    for i in range(15):\n",
    "        area=[\"seoul\",\"busan\",\"jeju\",\"daegu\",\"ulsan\",\"cheongju\",\"muan\",\"gwangju\",\"yeosu\"\n",
    "              ,\"yangyang\",\"pohang\",\"sacheon\",\"gunsan\",\"wonju\",\"incheon\"]\n",
    "        d=readJSON(area[i])\n",
    "        response=d['response'][0]\n",
    "        data=response['periods']\n",
    "    \n",
    "    \n",
    "        for j in range(len(data)):\n",
    "            temp=data[j]\n",
    "            date=dateutil.parser.parse(temp['dateTimeISO'])\n",
    "            temp2=pd.Series([date.year,date.month,date.day,date.hour,\"ARP\"+str(i+1),temp['tempC'],\n",
    "                         temp['humidity'],temp['dewpointC'],temp['windGustKTS'],temp['pressureMB']],\n",
    "                        index=['SDT_YY', 'SDT_MM', 'SDT_DD','STT','ARP','temp', 'hum', 'dew', 'windSpeed','hpa'])\n",
    "        \n",
    "            futureweather=futureweather.append(temp2,ignore_index=True)\n",
    "    \n",
    "    #fog 모델하고의 단위 맞추는 작업\n",
    "    futureweather['windSpeed']=futureweather['windSpeed'].astype(\"float\")*1852/3600\n",
    "    \n",
    "    futureweather.to_csv(\"newWeather.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일 저장하고 싶을때만 실행..\n",
    "storeWeather() \n",
    "\n",
    "# 날씨데이터 합치기---------------------\n",
    "\n",
    "we=pd.read_csv('newWeather.csv', encoding=\"cp949\")\n",
    "\n",
    "final= pd.merge(we, dly, on=['SDT_YY', 'SDT_MM', 'SDT_DD', 'ARP','STT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictFiveDays(df2):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "    for i in range(6):\n",
    "        update=[]\n",
    "        for j in range(24):\n",
    "            newTemp=0.0\n",
    "            newHum=0.0\n",
    "            newDew=0.0\n",
    "            newWindSpeed=0.0\n",
    "            newHpa=0.0\n",
    "            newDew=0.0\n",
    "            for k in range(5):\n",
    "                newTemp += df2[(df2['SDT_DD']==25+i-k+1) & (df2['STT']==j)]['temp']\n",
    "                newHum += df2[(df2['SDT_DD']==25+i-k+1) & (df2['STT']==j)]['hum']\n",
    "                newDew += df2[(df2['SDT_DD']==25+i-k+1) & (df2['STT']==j)]['dew']\n",
    "                newWindSpeed += df2[(df2['SDT_DD']==25+i-k+1) & (df2['STT']==j)]['windSpeed']\n",
    "                newHpa += df2[(df2['SDT_DD']==25+i-k+1) & (df2['STT']==j)]['hpa']\n",
    "                #newDew += df[(df['SDT_DD']==25+i-k+1) & (df['STT']==j)]['dew']\n",
    "#            print(df[(df['SDT_DD']==25+i-k) & (df['STT']==j)]['temp'])\n",
    "            df2[(df2['SDT_DD']==25+i) & (df2['STT']==j)]['temp']= newTemp / 5\n",
    "            df2[(df2['SDT_DD']==25+i) & (df2['STT']==j)]['hum']= newHum / 5\n",
    "            df2[(df2['SDT_DD']==25+i) & (df2['STT']==j)]['dew']= newDew / 5\n",
    "            df2[(df2['SDT_DD']==25+i) & (df2['STT']==j)]['windSpeed']= newWindSpeed / 5\n",
    "        \n",
    "    print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YY랑 DD삭제\n",
    "final.drop(columns=['SDT_YY', 'SDT_DD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일단 뺌\n",
    "final = one_hot_dummies(final, 'SDT_MM', 'SDT_DY', 'ARP', 'ODP', 'FLO',  'STT','ARPODP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def fogModel(df):\n",
    "    # 날씨 missing 값들은 0으로 대체\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    # 모델에서 쓰인 Scaling기법 적용\n",
    "    scaler = RobustScaler()\n",
    "    df[['hum', 'dew','temp','windSpeed']] = scaler.fit_transform(df[['hum', 'dew','temp','windSpeed']])\n",
    "    \n",
    "    # 저장된 모델 불러오기\n",
    "    clf_from_joblib = joblib.load('fogmodel.pkl') \n",
    "\n",
    "    # 지연 율 저장\n",
    "    fog_prob = clf_from_joblib.predict_proba(df)\n",
    "    \n",
    "\n",
    "    fog_column = []\n",
    "    # dly_rate에 지연율 저장\n",
    "    for i in fog_prob:\n",
    "        fog_column.append(i[1])\n",
    "        \n",
    "    \n",
    "    return fog_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df에서 fog관련 column만 함수에 넘김\n",
    "fog = final[[\"temp\",\"hum\",\"dew\",\"windSpeed\"]]\n",
    "\n",
    "fog_column = fogModel(fog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 안개 column 추가\n",
    "final['fog'] = fog_column\n",
    "\n",
    "# 안개 관련 column 제거\n",
    "final.drop(columns=['hum', 'dew','temp','windSpeed'], axis=1, inplace=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도착, 출발 데이터 분리\n",
    "final_A = final[final['AOD']=='A']\n",
    "final_D = final[final['AOD']=='D']\n",
    "\n",
    "# AOD column삭제 \n",
    "final_A = final_A.drop(['AOD'],axis = 1)\n",
    "final_D = final_D.drop(['AOD'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 아직 저장된 모델이 없음\n",
    "\n",
    "# df_A 모델 -----------------------------------------------\n",
    "\n",
    "# df_A 모델 불러오기\n",
    "predict_dealy_A_joblib = joblib.load('predict_delay_A.pkl') \n",
    "\n",
    "# DLY 저장하기\n",
    "dly_A = predict_dealy_A_joblib.predict(final_A)\n",
    "\n",
    "# DLY_RATE 저장하기\n",
    "dly_A_prob = predict_dealy_A_joblib.predict_proba(final_A)\n",
    "\n",
    "dly_rate_A = []\n",
    "\n",
    "# dly_rate에 지연율 저장\n",
    "for i in dly_A_prob:\n",
    "    dly_rate_A.append(i[1])\n",
    "    \n",
    "# DateFrame에 DLY, DLY_RATE추가\n",
    "final.loc[final['AOD']=='A','DLY'] = dly_A\n",
    "final.loc[final['AOD']=='A','DLY_RATE'] = dly_rate_A\n",
    "\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_D 모델 -----------------------------------------------\n",
    "\n",
    "# df_D 모델 불러오기\n",
    "predict_dealy_D_joblib = joblib.load('predict_delay_D.pkl') \n",
    "\n",
    "# DLY 저장하기\n",
    "dly_D = predict_dealy_D_joblib.predict(final_D)\n",
    "\n",
    "# DLY_RATE 저장하기\n",
    "dly_A_prob = predict_dealy_A_joblib.predict_proba(final_D)\n",
    "\n",
    "dly_rate_D = []\n",
    "\n",
    "# dly_rate에 지연율 저장\n",
    "for i in dly_D_prob:\n",
    "    dly_rate_D.append(i[1])\n",
    "    \n",
    "# DateFrame에 DLY, DLY_RATE추가\n",
    "final.loc[final['AOD']=='D','DLY'] = dly_D\n",
    "final.loc[final['AOD']=='D','DLY_RATE'] = dly_rate_D\n",
    "\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨을 1,0에서 -> Y,N으로 \n",
    "final.loc[final['DLY']==1,'DLY'] = 'Y'\n",
    "final.loc[final['DLY']==0,'DLY'] = 'N'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
